{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a219e22e",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "Naive because it assumes values are independant. In this set, we're doing it binomially since there are only two states for the target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc0d10dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>sentence_range</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "      <th>social_timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8601tu</th>\n",
       "      <td>ptsd</td>\n",
       "      <td>(15, 20)</td>\n",
       "      <td>He said he had not felt that way before, sugge...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1521614353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8lbrx9</th>\n",
       "      <td>assistance</td>\n",
       "      <td>(0, 5)</td>\n",
       "      <td>Hey there r/assistance, Not sure if this is th...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1527009817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9ch1zh</th>\n",
       "      <td>ptsd</td>\n",
       "      <td>(15, 20)</td>\n",
       "      <td>My mom then hit me with the newspaper and it s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1535935605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7rorpp</th>\n",
       "      <td>relationships</td>\n",
       "      <td>[5, 10]</td>\n",
       "      <td>until i met my new boyfriend, he is amazing, h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1516429555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9p2gbc</th>\n",
       "      <td>survivorsofabuse</td>\n",
       "      <td>[0, 5]</td>\n",
       "      <td>October is Domestic Violence Awareness Month a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1539809005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                subreddit sentence_range  \\\n",
       "post_id                                    \n",
       "8601tu               ptsd       (15, 20)   \n",
       "8lbrx9         assistance         (0, 5)   \n",
       "9ch1zh               ptsd       (15, 20)   \n",
       "7rorpp      relationships        [5, 10]   \n",
       "9p2gbc   survivorsofabuse         [0, 5]   \n",
       "\n",
       "                                                      text  label  confidence  \\\n",
       "post_id                                                                         \n",
       "8601tu   He said he had not felt that way before, sugge...      1         0.8   \n",
       "8lbrx9   Hey there r/assistance, Not sure if this is th...      0         1.0   \n",
       "9ch1zh   My mom then hit me with the newspaper and it s...      1         0.8   \n",
       "7rorpp   until i met my new boyfriend, he is amazing, h...      1         0.6   \n",
       "9p2gbc   October is Domestic Violence Awareness Month a...      1         0.8   \n",
       "\n",
       "         social_timestamp  \n",
       "post_id                    \n",
       "8601tu         1521614353  \n",
       "8lbrx9         1527009817  \n",
       "9ch1zh         1535935605  \n",
       "7rorpp         1516429555  \n",
       "9p2gbc         1539809005  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, log_loss, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "testFract = 0.1 #fraction of cases used for the test, the rest is used for training \n",
    "\n",
    "stressData = pandas.read_csv('Stress.csv', index_col='post_id')\n",
    "stressData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f89d157d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer( binary=True) #No stopwords used because stopwords were indicative of depression, so why not stress?\n",
    "\n",
    "stressData['text'].replace('[\\d][\\d]+', ' # ', regex=True, inplace=True)\n",
    "#only replacing numbers, not replacing caps or punctuation since caps and types of punctuation are used to express stress\n",
    "\n",
    "x = stressData.text\n",
    "y = stressData.label\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=testFract, train_size=1-testFract, random_state=1234)\n",
    "\n",
    "xTrain = tfidf.fit_transform(xTrain)\n",
    "xTest = tfidf.transform(xTest)\n",
    "\n",
    "naiveBayes = BernoulliNB()\n",
    "naiveBayes.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3ffb88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.7464788732394366\n",
      "precision =  0.6988636363636364\n",
      "recall =  0.8661971830985915\n",
      "f1 =  0.7735849056603774\n"
     ]
    }
   ],
   "source": [
    "# make predictions on the test data\n",
    "prediction = naiveBayes.predict(xTest)\n",
    "\n",
    "print('accuracy = ', accuracy_score(yTest, prediction))\n",
    "print('precision = ', precision_score(yTest, prediction))\n",
    "print('recall = ', recall_score(yTest, prediction))\n",
    "print('f1 = ', f1_score(yTest, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6a7501",
   "metadata": {},
   "source": [
    "# Logisic Regression\n",
    "Fucused on categorical classification instead of real number classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "502d1b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.7112676056338029\n",
      "precision =  0.6973684210526315\n",
      "recall =  0.7464788732394366\n",
      "f1 =  0.7210884353741497\n",
      "log loss = 0.527923688834927\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#train\n",
    "classifier = LogisticRegression(solver='lbfgs', class_weight='balanced')\n",
    "classifier.fit(xTrain, yTrain)\n",
    "\n",
    "predictionLR = classifier.predict(xTest)\n",
    "print('accuracy = ', accuracy_score(yTest, predictionLR))\n",
    "print('precision = ', precision_score(yTest, predictionLR))\n",
    "print('recall = ', recall_score(yTest, predictionLR))\n",
    "print('f1 = ', f1_score(yTest, predictionLR))\n",
    "print('log loss =', log_loss(yTest, classifier.predict_proba(xTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c3aba1",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "Four layers of \"neurons\" with an input, two hiiden, and an output layer. Hidden layers have been adjusted to fit as closely as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9353c443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=1e-05, hidden_layer_sizes=(6, 2), random_state=1,\n",
       "              solver='lbfgs')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tfidf.fit_transform(stressData.text)\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=testFract, train_size=1-testFract, random_state=1234)\n",
    "classifier = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(6, 2), random_state=1) # hidden layer sises adjusted to avoid overfitting\n",
    "classifier.fit(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fc22f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.7147887323943662\n",
      "precision =  0.7019867549668874\n",
      "recall =  0.7464788732394366\n",
      "f1 =  0.7235494880546076\n"
     ]
    }
   ],
   "source": [
    "predictionNN = classifier.predict(xTest)\n",
    "print('accuracy = ', accuracy_score(yTest, predictionNN))\n",
    "print('precision = ', precision_score(yTest, predictionNN))\n",
    "print('recall = ', recall_score(yTest, predictionNN))\n",
    "print('f1 = ', f1_score(yTest, predictionNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed49a82",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "All classifiers use the tfidf Vectorizer in binary mode with stopwords included. Stopwords are included because their inclusion helped scientists easier detect depression and it seems to work that way as well with stress. Binary vectorization is used because it seems what words used when stressed are more important than how many. Both these options have been experimented with, and seem to produce better results.\n",
    "Naive Bayes did the best in accuracy and recall, but did poor in precision, which means examples classified as stressed aren't really stressed. This means more cases are falsely categorised as stressed. In comparison neural networks did mediocre, with the exception of it being the best in precision. This may because groups of words easier indicate wherther a sentence is not stressful than independent ones, which the naive bayes assumes.\n",
    "Logistic Regression suprisingly did worse than Naive bayes, which I wouldn't have expected since this is a more catergorical process. Maybe the fact that the ouput of Naive Bayes is a real number helps with accuracy since stress level is somewhat uncertain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
